[package]
name = "vtcode-llm"
version = "0.0.1"
edition = "2024"
authors = ["vinhnx <vinhnx@users.noreply.github.com>"]
description = "Prototype extraction of VTCode's unified LLM client layer"
license = "MIT"
publish = false

[features]
default = [
    "anthropic",
    "deepseek",
    "google",
    "moonshot",
    "ollama",
    "openai",
    "openrouter",
    "xai",
    "zai",
    "functions",
]

anthropic = []
deepseek = []
google = []
moonshot = []
ollama = []
openai = []
openrouter = []
xai = []
zai = []
functions = []
telemetry = []
mock = ["dep:async-trait"]

[dependencies]
async-trait = { version = "0.1", optional = true }
vtcode-core = { path = "../vtcode-core" }

[dev-dependencies]
futures = "0.3"
