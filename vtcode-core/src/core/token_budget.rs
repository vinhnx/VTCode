//! Token budget management for context tracking
//!
//! This module implements token counting and budget tracking to manage
//! the attention budget of LLMs. It helps track token usage and monitor
//! thresholds for awareness of context size.

/// Maximum tokens allowed per tool response (token-based truncation limit)
pub const MAX_TOOL_RESPONSE_TOKENS: usize = 25_000;

use crate::utils::current_timestamp;
use anyhow::{Context, Result, anyhow};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::collections::VecDeque;
use std::fmt::Write;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokenizers::Tokenizer;
use tokio::sync::RwLock;
use tokio::task;
use tracing::{debug, warn};

use super::token_constants::{
    CODE_DETECTION_THRESHOLD, CODE_INDICATOR_CHARS, CODE_TOKEN_MULTIPLIER,
    LONG_WORD_CHAR_REDUCTION, LONG_WORD_SCALE_FACTOR, LONG_WORD_THRESHOLD, MIN_TOKEN_COUNT,
    TOKENS_PER_CHARACTER, TOKENS_PER_LINE,
};

/// Token budget configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenBudgetConfig {
    /// Maximum tokens allowed in context window
    pub max_context_tokens: usize,
    /// Threshold percentage to trigger warnings (0.0-1.0)
    pub warning_threshold: f64,
    /// Threshold percentage to trigger warnings (0.0-1.0)
    pub alert_threshold: f64,
    /// Model name for tokenizer selection
    pub model: String,
    /// Optional override for tokenizer identifier or local path
    pub tokenizer_id: Option<String>,
    /// Enable detailed token tracking
    pub detailed_tracking: bool,
}

impl Default for TokenBudgetConfig {
    fn default() -> Self {
        Self {
            max_context_tokens: 128_000,
            warning_threshold: 0.75,
            alert_threshold: 0.85,
            model: "gpt-5".to_owned(),
            tokenizer_id: None,
            detailed_tracking: false,
        }
    }
}

impl TokenBudgetConfig {
    /// Create config for specific model
    pub fn for_model(model: &str, max_tokens: usize) -> Self {
        Self {
            max_context_tokens: max_tokens,
            model: model.to_string(),
            tokenizer_id: None,
            ..Default::default()
        }
    }
}

/// Token usage statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenUsageStats {
    pub total_tokens: usize,
    pub system_prompt_tokens: usize,
    pub user_messages_tokens: usize,
    pub assistant_messages_tokens: usize,
    pub tool_results_tokens: usize,
    pub decision_ledger_tokens: usize,
    pub timestamp: u64,
}

impl TokenUsageStats {
    pub fn new() -> Self {
        Self {
            total_tokens: 0,
            system_prompt_tokens: 0,
            user_messages_tokens: 0,
            assistant_messages_tokens: 0,
            tool_results_tokens: 0,
            decision_ledger_tokens: 0,
            timestamp: current_timestamp(),
        }
    }

    /// Calculate percentage of max context used
    pub fn usage_percentage(&self, max_tokens: usize) -> f64 {
        self.usage_ratio(max_tokens) * 100.0
    }

    /// Calculate ratio (0.0-1.0) of max context used
    pub fn usage_ratio(&self, max_tokens: usize) -> f64 {
        if max_tokens == 0 || self.total_tokens == 0 {
            return 0.0;
        }
        self.total_tokens as f64 / max_tokens as f64
    }

    /// Check if alert threshold is exceeded
    pub fn exceeds_alert_threshold(&self, max_tokens: usize, threshold: f64) -> bool {
        self.usage_ratio(max_tokens) >= threshold
    }
}

impl Default for TokenUsageStats {
    fn default() -> Self {
        Self::new()
    }
}

/// Component types for detailed tracking
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum ContextComponent {
    SystemPrompt,
    UserMessage,
    AssistantMessage,
    ToolResult,
    DecisionLedger,
    ProjectGuidelines,
    FileContent,
}

#[derive(Clone)]
enum TokenCounter {
    HuggingFace(Arc<Tokenizer>),
    Approximate,
}

impl TokenCounter {
    fn huggingface(tokenizer: Tokenizer) -> Self {
        Self::HuggingFace(Arc::new(tokenizer))
    }

    fn count_tokens(&self, text: &str) -> Result<usize> {
        if text.is_empty() {
            return Ok(0);
        }

        match self {
            TokenCounter::HuggingFace(tokenizer) => {
                let encoding = tokenizer
                    .encode(text, true)
                    .map_err(|err| anyhow!("Tokenizer encode failed: {err}"))?;
                Ok(encoding.len())
            }
            TokenCounter::Approximate => Ok(approximate_token_count(text)),
        }
    }
}

enum TokenizerSpec {
    LocalFile(PathBuf),
    Pretrained {
        id: String,
        revision: Option<String>,
    },
}

/// Track applied max_tokens for tool calls
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MaxTokensUsage {
    pub tool_name: String,
    pub applied_max_tokens: Option<usize>,
    pub timestamp: u64,
    pub context: String, // Optional context about the tool call
}

/// Token budget manager
pub struct TokenBudgetManager {
    config: Arc<RwLock<TokenBudgetConfig>>,
    stats: Arc<RwLock<TokenUsageStats>>,
    component_tokens: Arc<RwLock<HashMap<String, usize>>>,
    tokenizer_cache: Arc<RwLock<Option<TokenCounter>>>,
    max_tokens_history: Arc<RwLock<VecDeque<MaxTokensUsage>>>,
}

impl TokenBudgetManager {
    /// Create a new token budget manager
    pub fn new(mut config: TokenBudgetConfig) -> Self {
        config.tokenizer_id = normalize_optional_string(config.tokenizer_id);

        Self {
            config: Arc::new(RwLock::new(config)),
            stats: Arc::new(RwLock::new(TokenUsageStats::new())),
            component_tokens: Arc::new(RwLock::new(HashMap::new())),
            tokenizer_cache: Arc::new(RwLock::new(None)),
            max_tokens_history: Arc::new(RwLock::new(VecDeque::new())),
        }
    }

    /// Ensure a tokenizer (or fallback counter) is available for the configured model
    async fn token_counter(&self) -> Result<TokenCounter> {
        if let Some(counter) = &*self.tokenizer_cache.read().await {
            return Ok(counter.clone());
        }

        let (model, tokenizer_id) = {
            let config = self.config.read().await;
            (config.model.clone(), config.tokenizer_id.clone())
        };
        let model_for_log = model.clone();
        let tokenizer_for_log = tokenizer_id.clone();

        let load_result =
            task::spawn_blocking(move || load_tokenizer_for_model(&model, tokenizer_id.as_deref()))
                .await
                .context("Tokenizer loading task failed")?;

        let counter = match load_result {
            Ok(tokenizer) => {
                debug!(
                    model = %model_for_log,
                    tokenizer = tokenizer_for_log
                        .as_deref()
                        .unwrap_or("<model-default>"),
                    "Initialized Hugging Face tokenizer",
                );
                TokenCounter::huggingface(tokenizer)
            }
            Err(error) => {
                warn!(
                    model = %model_for_log,
                    tokenizer = tokenizer_for_log
                        .as_deref()
                        .unwrap_or("<model-default>"),
                    error = %error,
                    "Falling back to heuristic token counter",
                );
                TokenCounter::Approximate
            }
        };

        let mut cache = self.tokenizer_cache.write().await;
        *cache = Some(counter.clone());
        Ok(counter)
    }

    /// Count tokens in text
    pub async fn count_tokens(&self, text: &str) -> Result<usize> {
        let counter = self.token_counter().await?;
        counter.count_tokens(text)
    }

    /// Update the token budget configuration at runtime.
    /// Resets the cached tokenizer when model-specific values change.
    pub async fn update_config(&self, mut new_config: TokenBudgetConfig) {
        new_config.tokenizer_id = normalize_optional_string(new_config.tokenizer_id);

        let mut config_guard = self.config.write().await;
        let model_changed = config_guard.model != new_config.model
            || config_guard.tokenizer_id != new_config.tokenizer_id
            || config_guard.max_context_tokens != new_config.max_context_tokens;

        *config_guard = new_config;
        drop(config_guard);

        if model_changed {
            let mut cache = self.tokenizer_cache.write().await;
            *cache = None;
        }
    }

    /// Count tokens with component tracking
    pub async fn count_tokens_for_component(
        &self,
        text: &str,
        component: ContextComponent,
        component_id: Option<&str>,
    ) -> Result<usize> {
        let token_count = self.count_tokens(text).await?;

        self.record_tokens_for_component(component, token_count, component_id)
            .await;

        Ok(token_count)
    }

    /// Record token usage for a component using a provided token count.
    pub async fn record_tokens_for_component(
        &self,
        component: ContextComponent,
        tokens: usize,
        component_id: Option<&str>,
    ) {
        if tokens == 0 {
            return;
        }

        let detailed_tracking = {
            let config = self.config.read().await;
            config.detailed_tracking
        };

        if detailed_tracking {
            let key = if let Some(id) = component_id {
                format!("{:?}:{}", component, id)
            } else {
                format!("{:?}", component)
            };
            let mut components = self.component_tokens.write().await;
            *components.entry(key).or_insert(0) += tokens;
        }

        let mut stats = self.stats.write().await;
        stats.total_tokens += tokens;

        match component {
            ContextComponent::SystemPrompt => stats.system_prompt_tokens += tokens,
            ContextComponent::UserMessage => stats.user_messages_tokens += tokens,
            ContextComponent::AssistantMessage => stats.assistant_messages_tokens += tokens,
            ContextComponent::ToolResult => stats.tool_results_tokens += tokens,
            ContextComponent::DecisionLedger => stats.decision_ledger_tokens += tokens,
            _ => {}
        }

        stats.timestamp = current_timestamp();
    }

    /// Get current usage statistics
    pub async fn get_stats(&self) -> TokenUsageStats {
        self.stats.read().await.clone()
    }

    /// Get component-level token breakdown
    pub async fn get_component_breakdown(&self) -> HashMap<String, usize> {
        self.component_tokens.read().await.clone()
    }
}

/// Threshold type for checking different alert levels
#[derive(Debug, Clone, Copy)]
pub enum ThresholdType {
    Warning,
    Alert,
}

impl TokenBudgetManager {
    /// Check if warning threshold is exceeded
    pub async fn is_warning_threshold_exceeded(&self) -> bool {
        self.exceeds_threshold(ThresholdType::Warning).await
    }

    /// Check if alert threshold is exceeded
    pub async fn is_alert_threshold_exceeded(&self) -> bool {
        self.exceeds_threshold(ThresholdType::Alert).await
    }

    /// Generic threshold check to eliminate duplicate code
    async fn exceeds_threshold(&self, threshold_type: ThresholdType) -> bool {
        let stats = self.stats.read().await;
        let config = self.config.read().await;
        let threshold = match threshold_type {
            ThresholdType::Warning => config.warning_threshold,
            ThresholdType::Alert => config.alert_threshold,
        };
        stats.exceeds_alert_threshold(config.max_context_tokens, threshold)
    }

    /// Get current usage percentage
    pub async fn usage_percentage(&self) -> f64 {
        self.get_usage_stats(|stats, max_tokens| stats.usage_percentage(max_tokens))
            .await
    }

    /// Get current usage ratio (0.0-1.0)
    pub async fn usage_ratio(&self) -> f64 {
        self.get_usage_stats(|stats, max_tokens| stats.usage_ratio(max_tokens))
            .await
    }

    /// Get remaining tokens in budget
    pub async fn remaining_tokens(&self) -> usize {
        self.get_usage_stats(|stats, max_tokens| max_tokens.saturating_sub(stats.total_tokens))
            .await
    }

    /// Generic method to get usage statistics without duplicate locking
    async fn get_usage_stats<R, F>(&self, calc_fn: F) -> R
    where
        F: FnOnce(&TokenUsageStats, usize) -> R,
    {
        let stats = self.stats.read().await;
        let config = self.config.read().await;
        calc_fn(&stats, config.max_context_tokens)
    }

    /// Reset token counts (e.g., after cleanup)
    pub async fn reset(&self) {
        let mut stats = self.stats.write().await;
        *stats = TokenUsageStats::new();
        let mut components = self.component_tokens.write().await;
        components.clear();
        debug!("Token budget reset");
    }

    /// Deduct tokens (after removal)
    pub async fn deduct_tokens(&self, component: ContextComponent, tokens: usize) {
        let mut stats = self.stats.write().await;
        stats.total_tokens = stats.total_tokens.saturating_sub(tokens);

        match component {
            ContextComponent::SystemPrompt => {
                stats.system_prompt_tokens = stats.system_prompt_tokens.saturating_sub(tokens)
            }
            ContextComponent::UserMessage => {
                stats.user_messages_tokens = stats.user_messages_tokens.saturating_sub(tokens)
            }
            ContextComponent::AssistantMessage => {
                stats.assistant_messages_tokens =
                    stats.assistant_messages_tokens.saturating_sub(tokens)
            }
            ContextComponent::ToolResult => {
                stats.tool_results_tokens = stats.tool_results_tokens.saturating_sub(tokens)
            }
            ContextComponent::DecisionLedger => {
                stats.decision_ledger_tokens = stats.decision_ledger_tokens.saturating_sub(tokens)
            }
            _ => {}
        }

        debug!("Deducted {} tokens from {:?}", tokens, component);
    }

    /// Generate a budget report
    pub async fn generate_report(&self) -> String {
        let stats = self.stats.read().await;
        let config = self.config.read().await;
        let components = self.component_tokens.read().await;

        let usage_ratio = stats.usage_ratio(config.max_context_tokens);
        let usage_pct = usage_ratio * 100.0;
        let remaining = config.max_context_tokens.saturating_sub(stats.total_tokens);

        let mut report = format!(
            "Token Budget Report\n\
             ==================\n\
             Total Tokens: {}/{} ({:.1}%)\n\
             Remaining: {} tokens\n\n\
             Breakdown by Category:\n\
             - System Prompt: {} tokens\n\
             - User Messages: {} tokens\n\
             - Assistant Messages: {} tokens\n\
             - Tool Results: {} tokens\n\
             - Decision Ledger: {} tokens\n",
            stats.total_tokens,
            config.max_context_tokens,
            usage_pct,
            remaining,
            stats.system_prompt_tokens,
            stats.user_messages_tokens,
            stats.assistant_messages_tokens,
            stats.tool_results_tokens,
            stats.decision_ledger_tokens
        );

        if config.detailed_tracking && !components.is_empty() {
            report.push_str("\nDetailed Component Tracking:\n");
            let mut sorted: Vec<_> = components.iter().collect();
            sorted.sort_by(|a, b| b.1.cmp(a.1));
            for (component, tokens) in sorted.iter().take(10) {
                let _ = writeln!(report, "  - {}: {} tokens", component, tokens);
            }
        }

        // Add max_tokens usage history
        let max_tokens_history = self.max_tokens_history.read().await;
        if !max_tokens_history.is_empty() {
            report.push_str("\nRecent max_tokens Usage:\n");
            for usage in max_tokens_history.iter().rev().take(10) {
                let applied = usage
                    .applied_max_tokens
                    .map(|n| n.to_string())
                    .unwrap_or_else(|| "None".to_owned());
                let _ = write!(
                    report,
                    "  - Tool: {} | Applied max_tokens: {} | Context: {}\n",
                    usage.tool_name, applied, usage.context
                );
            }
        }

        if usage_ratio >= config.alert_threshold {
            report.push_str("\nALERT: Alert threshold exceeded");
        } else if usage_ratio >= config.warning_threshold {
            report.push_str("\nWARNING: Approaching token limit");
        }

        report
    }

    /// Record max_tokens usage for a tool call
    pub async fn record_max_tokens_usage(
        &self,
        tool_name: &str,
        applied_max_tokens: Option<usize>,
        context: &str,
    ) {
        let mut history = self.max_tokens_history.write().await;

        history.push_front(MaxTokensUsage {
            tool_name: tool_name.to_string(),
            applied_max_tokens,
            timestamp: current_timestamp(),
            context: context.to_string(),
        });

        // Keep only the last 50 entries to prevent unlimited growth
        while history.len() > 50 {
            history.pop_back();
        }
    }

    /// Get recent max_tokens usage
    pub async fn get_recent_max_tokens_usage(&self) -> Vec<MaxTokensUsage> {
        let history = self.max_tokens_history.read().await;
        history.iter().cloned().collect()
    }

    /// Get summary of recent max_tokens usage
    pub async fn get_max_tokens_usage_summary(&self) -> String {
        let history = self.max_tokens_history.read().await;
        if history.is_empty() {
            return "No max_tokens usage recorded.".to_owned();
        }

        let mut summary = String::from("Recent max_tokens Usage Summary:\n");
        let _ = writeln!(summary, "Total recorded: {} tool calls", history.len());

        let mut with_max_tokens = 0;
        let mut total_max_tokens = 0;
        for usage in &*history {
            if let Some(max_tokens) = usage.applied_max_tokens {
                with_max_tokens += 1;
                total_max_tokens += max_tokens;
            }
        }

        if with_max_tokens > 0 {
            let _ = write!(
                summary,
                "Tool calls with max_tokens: {} (avg: {:.1})\n",
                with_max_tokens,
                total_max_tokens as f64 / with_max_tokens as f64
            );
        } else {
            summary.push_str("No tool calls with max_tokens applied.\n");
        }

        summary
    }
}

fn approximate_token_count(text: &str) -> usize {
    if text.trim().is_empty() {
        return 0;
    }

    // Improved approximation with content-aware heuristics:
    //
    // Empirical observations:
    // - Regular prose: ~1 token per 4-5 characters
    // - Code (has brackets/operators): ~1 token per 3 characters (more fragments)
    // - Whitespace-heavy (logs): ~1 token per 4.5 characters
    //
    // We use three independent methods and take their median for robustness:

    let word_count = text.split_whitespace().count();
    let char_count = text.chars().count();
    let line_count = text.lines().count();

    // Method 1: Character-based (conservative)
    // Accounts for punctuation, brackets, operators as separate tokens
    let char_tokens = (char_count as f64 / TOKENS_PER_CHARACTER).ceil() as usize;

    // Method 2: Word-based
    // Most words = 1 token, longer words above threshold = 2+ tokens
    // If no words, fall back to char-based estimate
    let word_tokens = if word_count == 0 {
        char_tokens
    } else {
        let avg_word_len = char_count / word_count;
        // Base: 1 token per word
        // Add extra tokens for longer average word length
        let extra_tokens =
            (avg_word_len.saturating_sub(LONG_WORD_THRESHOLD) / LONG_WORD_CHAR_REDUCTION).max(0);
        word_count + (word_count * extra_tokens / LONG_WORD_SCALE_FACTOR).max(0)
    };

    // Method 3: Line-based (for structured output like diffs, logs)
    // Most lines have ~TOKENS_PER_LINE tokens, empty lines: 1 token
    // More reliable for structured output (logs, diffs, stack traces)
    let non_empty_lines = text.lines().filter(|l| !l.trim().is_empty()).count();
    let empty_lines = line_count.saturating_sub(non_empty_lines);
    let line_tokens = non_empty_lines * TOKENS_PER_LINE + empty_lines;

    // Take median of three estimates for robustness
    // This prevents any single heuristic from being too aggressive or conservative
    let mut estimates = [char_tokens, word_tokens, line_tokens];
    estimates.sort_unstable();
    let median = estimates[1];

    // Apply content-specific adjustment
    // Code (has brackets/braces): increase estimate
    let bracket_count: usize = text
        .chars()
        .filter(|c| CODE_INDICATOR_CHARS.contains(*c))
        .count();
    let is_likely_code = bracket_count > (char_count / CODE_DETECTION_THRESHOLD);
    let result = if is_likely_code {
        (median as f64 * CODE_TOKEN_MULTIPLIER).ceil() as usize
    } else {
        median
    };

    result.max(MIN_TOKEN_COUNT)
}

fn load_tokenizer_for_model(model: &str, tokenizer_id: Option<&str>) -> Result<Tokenizer> {
    if let Some(spec) = tokenizer_id.and_then(resolve_tokenizer_spec) {
        return load_tokenizer_from_spec(&spec);
    }

    if let Some(spec) = resolve_tokenizer_spec(model) {
        return load_tokenizer_from_spec(&spec);
    }

    Err(anyhow!(
        "No tokenizer mapping available for model '{}'",
        model
    ))
}

fn load_tokenizer_from_spec(spec: &TokenizerSpec) -> Result<Tokenizer> {
    match spec {
        TokenizerSpec::LocalFile(path) => Tokenizer::from_file(path)
            .map_err(|err| anyhow!("Failed to load tokenizer from {}: {err}", path.display())),
        TokenizerSpec::Pretrained { id, revision } => {
            if let Some(rev) = revision {
                warn!(
                    "Tokenizer revision override '{}' is not supported; using default revision for '{}'",
                    rev, id
                );
            }
            Tokenizer::from_pretrained(id, None)
                .map_err(|err| anyhow!("Failed to load tokenizer '{id}': {err}"))
        }
    }
}

fn resolve_tokenizer_spec(identifier: &str) -> Option<TokenizerSpec> {
    let trimmed = identifier.trim();
    if trimmed.is_empty() {
        return None;
    }

    if let Some(path) = resolve_local_tokenizer_path(trimmed) {
        return Some(TokenizerSpec::LocalFile(path));
    }

    if trimmed.contains('/') {
        return Some(TokenizerSpec::Pretrained {
            id: trimmed.to_string(),
            revision: None,
        });
    }

    Some(TokenizerSpec::Pretrained {
        id: map_model_to_pretrained(trimmed).to_string(),
        revision: None,
    })
}

fn resolve_local_tokenizer_path(identifier: &str) -> Option<PathBuf> {
    let direct_path = Path::new(identifier);
    if direct_path.exists() {
        return Some(direct_path.to_path_buf());
    }

    let mut resource_path = PathBuf::from("resources/tokenizers");
    if identifier.ends_with(".json") {
        resource_path.push(identifier);
    } else {
        resource_path.push(format!("{identifier}.json"));
    }

    if resource_path.exists() {
        return Some(resource_path);
    }

    None
}

fn map_model_to_pretrained(model: &str) -> &'static str {
    let normalized = model.to_ascii_lowercase();

    if normalized.contains("gpt-4o") || normalized.contains("gpt-5") {
        "openai-community/gpt-4o-mini-tokenizer"
    } else if normalized.contains("gpt") {
        "openai-community/gpt2"
    } else if normalized.contains("gemini") {
        "google/gemma-2b"
    } else if normalized.contains("claude") {
        "Xenova/claude-3-haiku-20240307"
    } else if normalized.contains("glm") {
        "THUDM/chatglm3-6b"
    } else if normalized.contains("qwen") {
        "Qwen/Qwen1.5-7B-Chat"
    } else {
        "openai-community/gpt2"
    }
}

fn normalize_optional_string(value: Option<String>) -> Option<String> {
    value.and_then(|inner| {
        let trimmed = inner.trim();
        if trimmed.is_empty() {
            None
        } else {
            Some(trimmed.to_string())
        }
    })
}

impl Default for TokenBudgetManager {
    fn default() -> Self {
        Self::new(TokenBudgetConfig::default())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_token_counting() {
        let config = TokenBudgetConfig::default();
        let manager = TokenBudgetManager::new(config);

        let text = "Hello, world!";
        let count = manager.count_tokens(text).await.unwrap();
        assert!(count > 0);
    }

    #[tokio::test]
    async fn test_component_tracking() {
        let mut config = TokenBudgetConfig::default();
        config.detailed_tracking = true;
        let manager = TokenBudgetManager::new(config);

        let text = "This is a test message";
        let count = manager
            .count_tokens_for_component(text, ContextComponent::UserMessage, Some("msg1"))
            .await
            .unwrap();

        assert!(count > 0);

        let stats = manager.get_stats().await;
        assert_eq!(stats.user_messages_tokens, count);
    }

    #[tokio::test]
    async fn test_threshold_detection() {
        let manager = TokenBudgetManager::new(TokenBudgetConfig::default());

        // Add a chunk of tokens and capture the total usage
        let text = "word ".repeat(40);
        let total_tokens = manager
            .count_tokens_for_component(&text, ContextComponent::UserMessage, None)
            .await
            .unwrap();

        // Reconfigure thresholds so the current usage exceeds the alert threshold
        let mut updated_config = TokenBudgetConfig::default();
        updated_config.max_context_tokens = total_tokens.max(1);
        updated_config.alert_threshold = 0.5;
        manager.update_config(updated_config).await;

        assert!(manager.is_alert_threshold_exceeded().await);
    }

    #[tokio::test]
    async fn test_token_deduction() {
        let manager = TokenBudgetManager::new(TokenBudgetConfig::default());

        let text = "Hello, world!";
        let count = manager
            .count_tokens_for_component(text, ContextComponent::ToolResult, None)
            .await
            .unwrap();

        let initial_total = manager.get_stats().await.total_tokens;

        manager
            .deduct_tokens(ContextComponent::ToolResult, count)
            .await;

        let after_total = manager.get_stats().await.total_tokens;
        assert_eq!(after_total, initial_total - count);
    }

    #[tokio::test]
    async fn test_usage_ratio_updates_with_config_changes() {
        let mut config = TokenBudgetConfig::default();
        config.max_context_tokens = 100;
        let manager = TokenBudgetManager::new(config);

        manager
            .record_tokens_for_component(ContextComponent::SystemPrompt, 20, None)
            .await;

        let initial_ratio = manager.usage_ratio().await;
        assert!((initial_ratio - 0.2).abs() < f64::EPSILON);

        let mut new_config = TokenBudgetConfig::for_model("gpt-4", 200);
        new_config.warning_threshold = 0.6;
        new_config.alert_threshold = 0.8;
        manager.update_config(new_config).await;

        let updated_ratio = manager.usage_ratio().await;
        assert!((updated_ratio - 0.1).abs() < f64::EPSILON);
    }

    #[test]
    fn test_approximate_token_count_basic() {
        // Empty text
        assert_eq!(approximate_token_count(""), 0);
        assert_eq!(approximate_token_count("   \n  \t  "), 0);

        // Single word
        let single_word = approximate_token_count("hello");
        assert!(single_word > 0 && single_word <= 2);

        // Regular prose (typical)
        let prose = "The quick brown fox jumps over the lazy dog";
        let prose_count = approximate_token_count(prose);
        assert!(prose_count > 6 && prose_count < 12); // ~8-10 tokens expected

        // Code snippet (more tokens due to brackets/operators)
        let code = "fn foo() { let x = 42; return x + 1; }";
        let code_count = approximate_token_count(code);
        // Code should have more tokens than equivalent prose due to bracket detection
        assert!(code_count > prose_count || code_count == prose_count);
    }

    #[test]
    fn test_approximate_token_count_variance() {
        // Ensure different content types get different estimates
        let short_code = "var x = 1 + 2;";
        let short_prose = "The cat sat on the mat";

        let code_tokens = approximate_token_count(short_code);
        let prose_tokens = approximate_token_count(short_prose);

        // Both should be reasonable (not wildly off)
        assert!(code_tokens > 0 && prose_tokens > 0);
        // Code with operators should have comparable or more tokens
        assert!(code_tokens >= 2);
        assert!(prose_tokens >= 3);
    }

    #[test]
    fn test_approximate_token_count_scaling() {
        // Double the content should roughly double the tokens
        let short = "hello world";
        let long = "hello world hello world hello world hello world hello world";

        let short_tokens = approximate_token_count(short);
        let long_tokens = approximate_token_count(long);

        // Long should be significantly more
        assert!(long_tokens > short_tokens * 4);
    }

    #[test]
    fn test_approximate_token_count_with_code() {
        let code = "
fn main() {
    let vec = vec![1, 2, 3, 4, 5];
    for i in vec.iter() {
        println!(\"{}\", i);
    }
}
";
        let count = approximate_token_count(code);
        // Code should be detected and adjusted
        assert!(count > 20); // Reasonable estimate for a code block
    }

    #[test]
    fn test_approximate_token_count_with_logs() {
        let logs = "
[INFO] Starting process
[INFO] Loading config from /path/to/config.toml
[DEBUG] Configuration loaded: Config { debug: true }
[ERROR] Failed to connect: Connection refused
[ERROR] Retrying connection attempt 2/3
[WARN] Degraded mode activated
";
        let count = approximate_token_count(logs);
        // Log output should be estimated
        assert!(count > 30); // Reasonable estimate for logs
    }
}
